# HyperNeRF Website
Learning efficient and verifiable policy has been a challenging task in reinforcement learning (RL), especially in the visual RL setting with complex visual scenes. The deep neural networks have achieved competitive performance, yet the learned policies are over-parameterized black boxes that are hard to interpret and deploy efficiently. More recent symbolic RL frameworks have shown that high-level domain-specific programming logic can be designed to handle both policy learning and symbolic planning. However, these approaches heavily rely on human-coded primitives with little feature learning. Moreover, when applied to visual scenes, these methods use symbolic operators to process pixel-level inputs, which usually run into scalability issues and perform poorly when the images are high-dimensional and contain occlusion, partial observability, and complex background. In this work, we assume that the simplicity and scalability advantage of symbolic expressions could be maximized if they are used to deal with object-level abstractions, instead of pixel level, and if a proper neural-guided optimization could be developed instead of brute-force search. To this end, we present \textit{Differentiable Symbolic Expression Search} (DiffSES), a novel symbolic learning approach to discover discrete symbolic rules with partially differentiable optimization. Experiments demonstrate that in the visual RL settings, using a reduced amount of symbolic prior knowledge, DiffSES generates symbolic rules exhibiting superior interpretability as well as better scalability than current state-of-the-art symbolic RL methods. Our codes will be fully released upon acceptance.